---
title: "BranchGLM Vignette"
output: 
  rmarkdown::html_vignette:
    toc: TRUE
    number_sections: TRUE

vignette: >
  %\VignetteIndexEntry{BranchGLM Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Description

**BranchGLM** is a package for fitting GLMs and performing variable selection. 
Most functions in this package make use of RcppArmadillo 
and some of them can also make use of OpenMP to perform parallel computations. 
This vignette introduces the package, provides examples on how to use the 
main functions in the package and also briefly describes the methods employed by the 
functions.

# Installation

**BranchGLM** can be installed using the `install.packages()` function

```{r, eval = FALSE}

install.packages("BranchGLM")

```

It can also be installed via the `install_github()` function from the 
**devtools** package.

```{r, eval = FALSE}

devtools::install_github("JacobSeedorff21/BranchGLM")

```

# Fitting GLMs

- `BranchGLM()` allows fitting of gaussian, binomial, gamma, and poisson GLMs with a variety of links available.  
- Parallel computation can also be done to speed up the fitting process, but it is only useful for larger datasets.

## Optimization methods

- The optimization method can be specified, the default method is fisher scoring, but BFGS and L-BFGS are also available. 
- BFGS and L-BFGS typically perform better when there are many predictors in the model (at least 50 predictors), otherwise fisher scoring is typically faster.  
- The `grads` argument is for L-BFGS only and it is the number of gradients that are stored at a time and are used to approximate the inverse information. The default value for this is 10, but another common choice is 5.
- The `tol` argument controls how strict the convergence criteria are, lower values of this will lead to more accurate results, but may also be slower.
- The `method` argument is ignored for linear regression and the OLS solution is 
used.

## Examples

- An offset can be specified using `offset`, it should be a numeric vector.

```{r}
### Using mtcars

library(BranchGLM)

cars <- mtcars

### Fitting linear regression model with Fisher scoring

LinearFit <- BranchGLM(mpg ~ ., data = cars, family = "gaussian", link = "identity")

LinearFit

### Fitting gamma regression with inverse link with L-BFGS

GammaFit <- BranchGLM(mpg ~ ., data = cars, family = "gamma", link = "inverse",
                      method = "LBFGS", grads = 5)

GammaFit

```

## Useful functions

- **BranchGLM** also has many utility functions for GLMs such as
  - `coef()` to extract the coefficients
  - `logLik()` to extract the log likelihood
  - `AIC()` to extract the AIC
  - `BIC()` to extract the BIC
  - `predict()` to obtain predictions from the fitted model
- The coefficients, standard errors, wald test statistics, and p-values are stored in the `coefficients` slot of the fitted model

```{r}
### Predict method

predict(GammaFit)

### Accessing coefficients matrix

GammaFit$coefficients

```

# Performing variable selection

- Forward selection, backward elimination, and branch and bound selection can be done using `VariableSelection()`.
- `VariableSelection()` can accept either a `BranchGLM` object or a formula along with the data and the desired family and link to perform the variable selection.
- Available metrics are AIC and BIC, which are used to compare models and to select the best model.
- `VariableSelection()` returns the final model and some other information about the search.
- Note that `VariableSelection()` will properly handle interaction terms.
- `keep` can also be specified if any set of variables are desired to be kept in every model.

## Stepwise methods

- Forward selection and backward elimination are both stepwise variable selection methods.
- They are not guaranteed to find the best model or even a good model, but they are very fast.
- Forward selection is recommended if the number of variables is greater than the number of observations or if many of the larger models don't converge.
- Parallel computation can be used for the methods, but is generally only necessary 
for large datasets.

### Forward selection example

```{r}
### Forward selection with mtcars

VariableSelection(GammaFit, type = "forward")

```

### Backward elimination example

```{r}
### Backward elimination with mtcars

VariableSelection(GammaFit, type = "backward")

```

## Branch and bound

- The Branch and bound methods can be much slower than the stepwise methods, but 
they are guaranteed to find the best model.
- The branch and bound methods can be much faster than an exhaustive search and can also be made many times faster if parallel computation is used.
- One way to judge how much faster the branch and bound algorithm is compared to an exhaustive search is to look at the ratio of the number of models fit against the total number of possible models.
  - If this ratio is close to 1, then it is performing poorly and has to fit almost every model, if it is close to 0, then it is performing well and ruled out many models without having to fit them.

### Branch and bound example

- If `showprogress` is true, then progress of the branch and bound algorithm will be reported occasionally.
- Parallel computation can be used with these methods and can lead to very large speedups.

```{r}
### Branch and bound with mtcars

VariableSelection(GammaFit, type = "branch and bound", showprogress = FALSE)

### Can also use a formula and data

FormulaVS <- VariableSelection(mpg ~ . ,data = cars, family = "gamma", 
                               link = "inverse", type = "branch and bound",
                               showprogress = FALSE)

### Number of models fit divided by the number of possible models

FormulaVS$numchecked / 2^(length(FormulaVS$variables))

### Extracting final model

FormulaVS$finalmodel

```

## Using keep

- Specifying variables via `keep` will ensure that those variables are kept through the selection process.

```{r}
### Example of using keep

VariableSelection(mpg ~ . ,data = cars, family = "gamma", 
                               link = "inverse", type = "branch and bound",
                               keep = c("hp", "cyl"), metric = "AIC",
                               showprogress = FALSE)

```

## Convergence issues

- It is not recommended to use branch and bound if the upper models do not converge since it can make the algorithm very slow.
- Sometimes when using backwards selection and all the upper models that are tested 
do not converge, no final model can be selected.
- For these reasons, if there are convergence issues it is recommended to use forward selection.

# Utility functions for binomial GLMs

- **BranchGLM** also has some utility functions for binomial GLMs
  - `Table()` creates a confusion matrix based on the predicted classes and observed classes
  - `ROC()` creates an ROC curve which can be plotted with `plot()`
  - `AUC()` and `Cindex()` calculate the area under the ROC curve
  - `MultipleROCCurves()` allows for the plotting of multiple ROC curves on the same plot

## Table

```{r}
### Predicting if a car gets at least 18 mpg

catData <- ToothGrowth

catFit <- BranchGLM(supp ~ ., data = catData, family = "binomial", link = "logit")

Table(catFit)

```

## ROC

```{r}

catROC <- ROC(catFit)

plot(catROC, main = "ROC Curve", col = "indianred")

```

## Cindex/AUC

```{r}

Cindex(catFit)

AUC(catFit)

```

## MultipleROCPlots

```{r, fig.width = 4, fig.height = 4}
### Showing ROC plots for logit, probit, and cloglog

probitFit <- BranchGLM(supp ~ . ,data = catData, family = "binomial", 
                       link = "probit")

cloglogFit <- BranchGLM(supp ~ . ,data = catData, family = "binomial", 
                       link = "cloglog")

MultipleROCCurves(catROC, ROC(probitFit), ROC(cloglogFit), 
                  names = c("Logistic ROC", "Probit ROC", "Cloglog ROC"))

```

## Using predictions

- For each of the methods used in this section predicted probabilities and observed
classes can also be supplied instead of the `BranchGLM` object.

```{r}

preds <- predict(catFit)

Table(preds, catData$supp)

AUC(preds, catData$supp)

ROC(preds, catData$supp) |> plot(main = "ROC Curve", col = "deepskyblue")

```
